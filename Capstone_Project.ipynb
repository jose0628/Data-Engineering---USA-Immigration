{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### USA-Immigration Data Warehouse \n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "Many people travel to USA for different purposes, the TSA (Transport Security Administration) is interested to know in depth the immigration patterns in a monthly basis by the airport based on different factors, such as immigration data, temperature, US Demographics and Airport Codes. This project provides a data warehouse, which will allow different TSA members to access curated data that can be use for making reports and deeper analytics insights related to traveler patterns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import udf, rand\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import configparser\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Scope the Project and Datasets\n",
    "\n",
    "#### Scope \n",
    "In order to achieve a data warehouse, I developed a data pipeline that Extract - Transform - Load data into a data warehouse. This will allow data analysts to consume the data and provide deeper data insights. The project involves different cloud technologies, such as Redshift (data warehouse), pySpark (read some datasets) and Apache AirFlow for data pipeline orchestration\n",
    "\n",
    "#### Datasets\n",
    "\n",
    "* I94 Immigration Data: This data comes from the US National Tourism and Trade Office and we will use the 2016 data. Each report contains international visitor arrival statistics by world regions and select countries such as type of visa, mode of transportation, age groups, states visited, the top ports of entry, etc. [Source](https://travel.trade.gov/research/reports/i94/historical/2016.html)\n",
    "\n",
    "* World Temperature Data: This dataset came from Kaggle and contains a compilation of global temperatures since 1750. In this case we will focus on the dataset **GlobalLandTemperatureByCity.csv**, which contains: AverageTemperature, AverageTemperatureUncertainty, City, Country, Latitude, Longitude. [Source](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data).\n",
    "\n",
    "\n",
    "* U.S. City Demographic Data: This dataset contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. This data comes from the US Census Bureau's 2015 American Community Survey. [Source](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/)\n",
    "\n",
    "* Airport Code Table: The airport codes may refer to either IATA airport code, a three-letter code which is used in passenger reservation, ticketing and baggage-handling systems, or the ICAO airport code which is a four letter code used by ATC systems and for airports that do not have an IATA airport code. Airport codes from around the world. Downloaded from public domain source http://ourairports.com/data/ who compiled this data from multiple different sources. [Source](https://datahub.io/core/airport-codes#data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Datasets Gathering, Exploration & Analysis\n",
    "\n",
    "\n",
    "### I-94 Immigration Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset 3096313\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5748517.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20582.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>QF</td>\n",
       "      <td>9.495387e+10</td>\n",
       "      <td>00011</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5748518.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NV</td>\n",
       "      <td>20591.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>VA</td>\n",
       "      <td>9.495562e+10</td>\n",
       "      <td>00007</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5748519.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>20582.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>9.495641e+10</td>\n",
       "      <td>00040</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5748520.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>20588.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>9.495645e+10</td>\n",
       "      <td>00040</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5748521.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>20588.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>DL</td>\n",
       "      <td>9.495639e+10</td>\n",
       "      <td>00040</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5748522.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>20579.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>NZ</td>\n",
       "      <td>9.498180e+10</td>\n",
       "      <td>00010</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5748523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>20586.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>NZ</td>\n",
       "      <td>9.497969e+10</td>\n",
       "      <td>00010</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5748524.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>20586.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1975.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>NZ</td>\n",
       "      <td>9.497975e+10</td>\n",
       "      <td>00010</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5748525.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>HOU</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>20581.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>NZ</td>\n",
       "      <td>9.497325e+10</td>\n",
       "      <td>00028</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5748526.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20581.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>NZ</td>\n",
       "      <td>9.501355e+10</td>\n",
       "      <td>00002</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5748527.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>NEW</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20576.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1972.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>UA</td>\n",
       "      <td>9.493829e+10</td>\n",
       "      <td>01215</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5748528.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>20575.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1977.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>CM</td>\n",
       "      <td>9.501810e+10</td>\n",
       "      <td>00472</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5748529.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>VA</td>\n",
       "      <td>20596.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1978.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>CM</td>\n",
       "      <td>9.492490e+10</td>\n",
       "      <td>00488</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5748530.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20577.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>10292016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>CM</td>\n",
       "      <td>9.492648e+10</td>\n",
       "      <td>00302</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5748531.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20577.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1978.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>CM</td>\n",
       "      <td>9.492629e+10</td>\n",
       "      <td>00302</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode  \\\n",
       "0   5748517.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "1   5748518.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "2   5748519.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "3   5748520.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "4   5748521.0  2016.0     4.0   245.0   438.0     LOS  20574.0      1.0   \n",
       "5   5748522.0  2016.0     4.0   245.0   464.0     HHW  20574.0      1.0   \n",
       "6   5748523.0  2016.0     4.0   245.0   464.0     HHW  20574.0      1.0   \n",
       "7   5748524.0  2016.0     4.0   245.0   464.0     HHW  20574.0      1.0   \n",
       "8   5748525.0  2016.0     4.0   245.0   464.0     HOU  20574.0      1.0   \n",
       "9   5748526.0  2016.0     4.0   245.0   464.0     LOS  20574.0      1.0   \n",
       "10  5748527.0  2016.0     4.0   245.0   504.0     NEW  20574.0      1.0   \n",
       "11  5748528.0  2016.0     4.0   245.0   504.0     LOS  20574.0      1.0   \n",
       "12  5748529.0  2016.0     4.0   245.0   504.0     WAS  20574.0      1.0   \n",
       "13  5748530.0  2016.0     4.0   245.0   504.0     LOS  20574.0      1.0   \n",
       "14  5748531.0  2016.0     4.0   245.0   504.0     LOS  20574.0      1.0   \n",
       "\n",
       "   i94addr  depdate   ...     entdepu  matflag  biryear   dtaddto gender  \\\n",
       "0       CA  20582.0   ...        None        M   1976.0  10292016      F   \n",
       "1       NV  20591.0   ...        None        M   1984.0  10292016      F   \n",
       "2       WA  20582.0   ...        None        M   1987.0  10292016      M   \n",
       "3       WA  20588.0   ...        None        M   1987.0  10292016      F   \n",
       "4       WA  20588.0   ...        None        M   1988.0  10292016      M   \n",
       "5       HI  20579.0   ...        None        M   1959.0  10292016      M   \n",
       "6       HI  20586.0   ...        None        M   1950.0  10292016      F   \n",
       "7       HI  20586.0   ...        None        M   1975.0  10292016      F   \n",
       "8       FL  20581.0   ...        None        M   1989.0  10292016      M   \n",
       "9       CA  20581.0   ...        None        M   1990.0  10292016      F   \n",
       "10      MA  20576.0   ...        None        M   1972.0  10292016      M   \n",
       "11    None  20575.0   ...        None        M   1977.0  10292016      M   \n",
       "12      VA  20596.0   ...        None        M   1978.0  10292016      M   \n",
       "13      CA  20577.0   ...        None        M   1960.0  10292016      F   \n",
       "14      CA  20577.0   ...        None        M   1978.0  10282016      M   \n",
       "\n",
       "   insnum airline        admnum  fltno visatype  \n",
       "0    None      QF  9.495387e+10  00011       B1  \n",
       "1    None      VA  9.495562e+10  00007       B1  \n",
       "2    None      DL  9.495641e+10  00040       B1  \n",
       "3    None      DL  9.495645e+10  00040       B1  \n",
       "4    None      DL  9.495639e+10  00040       B1  \n",
       "5    None      NZ  9.498180e+10  00010       B2  \n",
       "6    None      NZ  9.497969e+10  00010       B2  \n",
       "7    None      NZ  9.497975e+10  00010       B2  \n",
       "8    None      NZ  9.497325e+10  00028       B2  \n",
       "9    None      NZ  9.501355e+10  00002       B2  \n",
       "10   None      UA  9.493829e+10  01215       B2  \n",
       "11   None      CM  9.501810e+10  00472       B2  \n",
       "12   None      CM  9.492490e+10  00488       B2  \n",
       "13   None      CM  9.492648e+10  00302       B2  \n",
       "14   None      CM  9.492629e+10  00302       B2  \n",
       "\n",
       "[15 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imm_data = spark.read.parquet(\"dags/datasets/sas_data\")\n",
    "print(\"Size of the dataset\", imm_data.count())\n",
    "imm_data.limit(15).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<p style=\"color:blue;\"> By looking at this SAS information, it represents a person record per row with the main details of entry and exit in USA, this is very insightful and it will become in the main fact table for our project. I also extract the schema of the SAS file to understand the data types and is NULL values are allowed in certain columns </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imm_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<p style=\"color:blue;\"> It is important to understand the number of columns and the value types to later consider them in the sql tables </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15 entries, 0 to 14\n",
      "Data columns (total 28 columns):\n",
      "cicid       15 non-null float64\n",
      "i94yr       15 non-null float64\n",
      "i94mon      15 non-null float64\n",
      "i94cit      15 non-null float64\n",
      "i94res      15 non-null float64\n",
      "i94port     15 non-null object\n",
      "arrdate     15 non-null float64\n",
      "i94mode     15 non-null float64\n",
      "i94addr     14 non-null object\n",
      "depdate     15 non-null float64\n",
      "i94bir      15 non-null float64\n",
      "i94visa     15 non-null float64\n",
      "count       15 non-null float64\n",
      "dtadfile    15 non-null object\n",
      "visapost    15 non-null object\n",
      "occup       0 non-null object\n",
      "entdepa     15 non-null object\n",
      "entdepd     15 non-null object\n",
      "entdepu     0 non-null object\n",
      "matflag     15 non-null object\n",
      "biryear     15 non-null float64\n",
      "dtaddto     15 non-null object\n",
      "gender      15 non-null object\n",
      "insnum      0 non-null object\n",
      "airline     15 non-null object\n",
      "admnum      15 non-null float64\n",
      "fltno       15 non-null object\n",
      "visatype    15 non-null object\n",
      "dtypes: float64(13), object(15)\n",
      "memory usage: 3.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_imm = imm_data.limit(15).toPandas()\n",
    "df_imm.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<p style=\"color:blue;\"> Given the size of this dataset and the limited amount of memory in the workspace, I just wanted to have an idea if some columns have potential null values as percentage with 10000 records </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cicid       0.0000\n",
       "i94yr       0.0000\n",
       "i94mon      0.0000\n",
       "i94cit      0.0000\n",
       "i94res      0.0000\n",
       "i94port     0.0000\n",
       "arrdate     0.0000\n",
       "i94mode     0.0000\n",
       "i94addr     0.0400\n",
       "depdate     0.0517\n",
       "i94bir      0.0000\n",
       "i94visa     0.0000\n",
       "count       0.0000\n",
       "dtadfile    0.0000\n",
       "visapost    0.4918\n",
       "occup       0.9936\n",
       "entdepa     0.0000\n",
       "entdepd     0.0517\n",
       "entdepu     0.9999\n",
       "matflag     0.0517\n",
       "biryear     0.0000\n",
       "dtaddto     0.0000\n",
       "gender      0.0422\n",
       "insnum      1.0000\n",
       "airline     0.0000\n",
       "admnum      0.0000\n",
       "fltno       0.0000\n",
       "visatype    0.0000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imm_all = imm_data.limit(10000).toPandas()\n",
    "df_imm_all.isnull().sum()/df_imm_all.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<p style=\"color:blue;\"> We can notice that some columns are incomplete such as depdate, occup, gender records for example. However, it is not critical to have those values empty in this particular dataset </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### I94_SAS_Labels_Description.SAS\n",
    "The previous dataset in SAS contains an additional labels and descriptions, which can be complemented as dimensions. However, the SAS file needs to be parsed in order to extract the codes. In particular for the project, I am interested in the following codes:\n",
    "\n",
    "- Country\n",
    "- Port\n",
    "- Mode\n",
    "- Addr\n",
    "- Type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sas_program_file_value_parser(sas_source_file, value, columns):\n",
    "    \"\"\"Parses SAS Program file to return value as pandas dataframe\n",
    "    Args:\n",
    "        sas_source_file (str): SAS source code file.\n",
    "        value (str): sas value to extract.\n",
    "        columns (list): list of 2 containing column names.\n",
    "    Return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    file_string = ''\n",
    "    \n",
    "    with open(sas_source_file) as f:\n",
    "        file_string = f.read()\n",
    "    \n",
    "    file_string = file_string[file_string.index(value):]\n",
    "    file_string = file_string[:file_string.index(';')]\n",
    "    \n",
    "    line_list = file_string.split('\\n')[1:]\n",
    "    codes = []\n",
    "    values = []\n",
    "    \n",
    "    for line in line_list:\n",
    "        \n",
    "        if '=' in line:\n",
    "            code, val = line.split('=')\n",
    "            code = code.strip()\n",
    "            val = val.strip()\n",
    "\n",
    "            if code[0] == \"'\":\n",
    "                code = code[1:-1]\n",
    "\n",
    "            if val[0] == \"'\":\n",
    "                val = val[1:-1]\n",
    "\n",
    "            codes.append(code)\n",
    "            values.append(val)\n",
    "        \n",
    "            \n",
    "    return pd.DataFrame(zip(codes,values), columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94cit_res = sas_program_file_value_parser('dags/datasets/I94_SAS_Labels_Descriptions.SAS', 'i94cntyl', ['code', 'country'])\n",
    "i94port = sas_program_file_value_parser('dags/datasets/I94_SAS_Labels_Descriptions.SAS', 'i94prtl', ['code', 'port'])\n",
    "i94mode = sas_program_file_value_parser('dags/datasets/I94_SAS_Labels_Descriptions.SAS', 'i94model', ['code', 'mode'])\n",
    "i94addr = sas_program_file_value_parser('dags/datasets/I94_SAS_Labels_Descriptions.SAS', 'i94addrl', ['code', 'addr'])\n",
    "i94visa = sas_program_file_value_parser('dags/datasets/I94_SAS_Labels_Descriptions.SAS', 'I94VISA', ['code', 'type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### World Temperature Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset:  8599212\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1744-04-01</td>\n",
       "      <td>5.788</td>\n",
       "      <td>3.624</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1744-05-01</td>\n",
       "      <td>10.644</td>\n",
       "      <td>1.283</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1744-06-01</td>\n",
       "      <td>14.051</td>\n",
       "      <td>1.347</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1744-07-01</td>\n",
       "      <td>16.082</td>\n",
       "      <td>1.396</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1744-08-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "5  1744-04-01               5.788                          3.624  Århus   \n",
       "6  1744-05-01              10.644                          1.283  Århus   \n",
       "7  1744-06-01              14.051                          1.347  Århus   \n",
       "8  1744-07-01              16.082                          1.396  Århus   \n",
       "9  1744-08-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  \n",
       "5  Denmark   57.05N    10.33E  \n",
       "6  Denmark   57.05N    10.33E  \n",
       "7  Denmark   57.05N    10.33E  \n",
       "8  Denmark   57.05N    10.33E  \n",
       "9  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp_data = pd.read_csv('dags/datasets/GlobalLandTemperaturesByCity.csv')\n",
    "print(\"Size of the dataset: \", len(df_temp_data))\n",
    "df_temp_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<p style=\"color:blue;\"> In this dataset, I am interested to know potential unique values on the dataset </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt False\n",
      "AverageTemperature False\n",
      "AverageTemperatureUncertainty False\n",
      "City False\n",
      "Country False\n",
      "Latitude False\n",
      "Longitude False\n"
     ]
    }
   ],
   "source": [
    "for col in df_temp_data:\n",
    "    print(col, df_temp_data[col].is_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<p style=\"color:blue;\"> The dt clumns is not unique or datetime, which make sense, since we have diferent countries. As a rule of thumb, since this dataset specifies temperatures. If the AverageTemperature and AverageTemperatureUncertainty are NaN, we can actually delete those entries and keep only values with temperature measurements. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8599212 entries, 0 to 8599211\n",
      "Data columns (total 7 columns):\n",
      "dt                               object\n",
      "AverageTemperature               float64\n",
      "AverageTemperatureUncertainty    float64\n",
      "City                             object\n",
      "Country                          object\n",
      "Latitude                         object\n",
      "Longitude                        object\n",
      "dtypes: float64(2), object(5)\n",
      "memory usage: 459.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_temp_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<p style=\"color:blue;\"> This just provided us an idea of the type of data in the dataset, most of the objects seems to be text </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt                               0.000000\n",
       "AverageTemperature               0.042345\n",
       "AverageTemperatureUncertainty    0.042345\n",
       "City                             0.000000\n",
       "Country                          0.000000\n",
       "Latitude                         0.000000\n",
       "Longitude                        0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp_data.isnull().sum()/df_temp_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<p style=\"color:blue;\"> The proportion of NULL values is quite small compared with the size of the dataset, as an idea we can remove the NULL temperature values, since those rows do not bring too much value. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### USA City Demographics Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset:  2891\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Peoria</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>33.1</td>\n",
       "      <td>56229.0</td>\n",
       "      <td>62432.0</td>\n",
       "      <td>118661</td>\n",
       "      <td>6634.0</td>\n",
       "      <td>7517.0</td>\n",
       "      <td>2.40</td>\n",
       "      <td>IL</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>1343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Avondale</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>29.1</td>\n",
       "      <td>38712.0</td>\n",
       "      <td>41971.0</td>\n",
       "      <td>80683</td>\n",
       "      <td>4815.0</td>\n",
       "      <td>8355.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>11592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>West Covina</td>\n",
       "      <td>California</td>\n",
       "      <td>39.8</td>\n",
       "      <td>51629.0</td>\n",
       "      <td>56860.0</td>\n",
       "      <td>108489</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>37038.0</td>\n",
       "      <td>3.56</td>\n",
       "      <td>CA</td>\n",
       "      <td>Asian</td>\n",
       "      <td>32716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>O'Fallon</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>36.0</td>\n",
       "      <td>41762.0</td>\n",
       "      <td>43270.0</td>\n",
       "      <td>85032</td>\n",
       "      <td>5783.0</td>\n",
       "      <td>3269.0</td>\n",
       "      <td>2.77</td>\n",
       "      <td>MO</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>2583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>High Point</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>35.5</td>\n",
       "      <td>51751.0</td>\n",
       "      <td>58077.0</td>\n",
       "      <td>109828</td>\n",
       "      <td>5204.0</td>\n",
       "      <td>16315.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>NC</td>\n",
       "      <td>Asian</td>\n",
       "      <td>11060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City           State  Median Age  Male Population  \\\n",
       "0     Silver Spring        Maryland        33.8          40601.0   \n",
       "1            Quincy   Massachusetts        41.0          44129.0   \n",
       "2            Hoover         Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga      California        34.5          88127.0   \n",
       "4            Newark      New Jersey        34.6         138040.0   \n",
       "5            Peoria        Illinois        33.1          56229.0   \n",
       "6          Avondale         Arizona        29.1          38712.0   \n",
       "7       West Covina      California        39.8          51629.0   \n",
       "8          O'Fallon        Missouri        36.0          41762.0   \n",
       "9        High Point  North Carolina        35.5          51751.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "5            62432.0            118661              6634.0        7517.0   \n",
       "6            41971.0             80683              4815.0        8355.0   \n",
       "7            56860.0            108489              3800.0       37038.0   \n",
       "8            43270.0             85032              5783.0        3269.0   \n",
       "9            58077.0            109828              5204.0       16315.0   \n",
       "\n",
       "   Average Household Size State Code                               Race  Count  \n",
       "0                    2.60         MD                 Hispanic or Latino  25924  \n",
       "1                    2.39         MA                              White  58723  \n",
       "2                    2.58         AL                              Asian   4759  \n",
       "3                    3.18         CA          Black or African-American  24437  \n",
       "4                    2.73         NJ                              White  76402  \n",
       "5                    2.40         IL  American Indian and Alaska Native   1343  \n",
       "6                    3.18         AZ          Black or African-American  11592  \n",
       "7                    3.56         CA                              Asian  32716  \n",
       "8                    2.77         MO                 Hispanic or Latino   2583  \n",
       "9                    2.65         NC                              Asian  11060  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city_dem_data = pd.read_csv('dags/datasets/us-cities-demographics.csv', sep=';')\n",
    "print(\"Size of dataset: \", len(df_city_dem_data))\n",
    "df_city_dem_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2891 entries, 0 to 2890\n",
      "Data columns (total 12 columns):\n",
      "City                      2891 non-null object\n",
      "State                     2891 non-null object\n",
      "Median Age                2891 non-null float64\n",
      "Male Population           2888 non-null float64\n",
      "Female Population         2888 non-null float64\n",
      "Total Population          2891 non-null int64\n",
      "Number of Veterans        2878 non-null float64\n",
      "Foreign-born              2878 non-null float64\n",
      "Average Household Size    2875 non-null float64\n",
      "State Code                2891 non-null object\n",
      "Race                      2891 non-null object\n",
      "Count                     2891 non-null int64\n",
      "dtypes: float64(6), int64(2), object(4)\n",
      "memory usage: 271.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_city_dem_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<p style=\"color:blue;\"> I explore again the type of values in order to define them later my database tables </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City                      0.000000\n",
       "State                     0.000000\n",
       "Median Age                0.000000\n",
       "Male Population           0.001038\n",
       "Female Population         0.001038\n",
       "Total Population          0.000000\n",
       "Number of Veterans        0.004497\n",
       "Foreign-born              0.004497\n",
       "Average Household Size    0.005534\n",
       "State Code                0.000000\n",
       "Race                      0.000000\n",
       "Count                     0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city_dem_data.isnull().sum()/df_city_dem_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<p style=\"color:blue;\"> We get a proportion of the potential NULL values in the dataset, in order to undertand which columns may need a data test. In this dataset, I would not remove the incomplete rows, since it offers relevant informations in other columns </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Airport Code Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00AS</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Fulton Airport</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-OK</td>\n",
       "      <td>Alex</td>\n",
       "      <td>00AS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AS</td>\n",
       "      <td>-97.8180194, 34.9428028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00AZ</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Cordes Airport</td>\n",
       "      <td>3810.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AZ</td>\n",
       "      <td>Cordes</td>\n",
       "      <td>00AZ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AZ</td>\n",
       "      <td>-112.16500091552734, 34.305599212646484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00CA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Goldstone /Gts/ Airport</td>\n",
       "      <td>3038.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-CA</td>\n",
       "      <td>Barstow</td>\n",
       "      <td>00CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00CA</td>\n",
       "      <td>-116.888000488, 35.350498199499995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00CL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Williams Ag Airport</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-CA</td>\n",
       "      <td>Biggs</td>\n",
       "      <td>00CL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00CL</td>\n",
       "      <td>-121.763427, 39.427188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00CN</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Kitchen Creek Helibase Heliport</td>\n",
       "      <td>3350.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-CA</td>\n",
       "      <td>Pine Valley</td>\n",
       "      <td>00CN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00CN</td>\n",
       "      <td>-116.4597417, 32.7273736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "5  00AS  small_airport                      Fulton Airport        1100.0   \n",
       "6  00AZ  small_airport                      Cordes Airport        3810.0   \n",
       "7  00CA  small_airport             Goldstone /Gts/ Airport        3038.0   \n",
       "8  00CL  small_airport                 Williams Ag Airport          87.0   \n",
       "9  00CN       heliport     Kitchen Creek Helibase Heliport        3350.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "5       NaN          US      US-OK          Alex     00AS       NaN   \n",
       "6       NaN          US      US-AZ        Cordes     00AZ       NaN   \n",
       "7       NaN          US      US-CA       Barstow     00CA       NaN   \n",
       "8       NaN          US      US-CA         Biggs     00CL       NaN   \n",
       "9       NaN          US      US-CA   Pine Valley     00CN       NaN   \n",
       "\n",
       "  local_code                              coordinates  \n",
       "0        00A       -74.93360137939453, 40.07080078125  \n",
       "1       00AA                   -101.473911, 38.704022  \n",
       "2       00AK              -151.695999146, 59.94919968  \n",
       "3       00AL    -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                      -91.254898, 35.6087  \n",
       "5       00AS                  -97.8180194, 34.9428028  \n",
       "6       00AZ  -112.16500091552734, 34.305599212646484  \n",
       "7       00CA       -116.888000488, 35.350498199499995  \n",
       "8       00CL                   -121.763427, 39.427188  \n",
       "9       00CN                 -116.4597417, 32.7273736  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport_code_data = pd.read_csv('dags/datasets/airport-codes_csv.csv')\n",
    "df_airport_code_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ident True\n",
      "type False\n",
      "name False\n",
      "elevation_ft False\n",
      "continent False\n",
      "iso_country False\n",
      "iso_region False\n",
      "municipality False\n",
      "gps_code False\n",
      "iata_code False\n",
      "local_code False\n",
      "coordinates False\n"
     ]
    }
   ],
   "source": [
    "for col in df_airport_code_data:\n",
    "    print(col, df_airport_code_data[col].is_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<p style=\"color:blue;\"> Here the indent column is unique and it serves as primary key </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55075 entries, 0 to 55074\n",
      "Data columns (total 12 columns):\n",
      "ident           55075 non-null object\n",
      "type            55075 non-null object\n",
      "name            55075 non-null object\n",
      "elevation_ft    48069 non-null float64\n",
      "continent       27356 non-null object\n",
      "iso_country     54828 non-null object\n",
      "iso_region      55075 non-null object\n",
      "municipality    49399 non-null object\n",
      "gps_code        41030 non-null object\n",
      "iata_code       9189 non-null object\n",
      "local_code      28686 non-null object\n",
      "coordinates     55075 non-null object\n",
      "dtypes: float64(1), object(11)\n",
      "memory usage: 5.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_airport_code_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<p style=\"color:blue;\"> Here the object types seems to be dominant by text types </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ident           0.000000\n",
       "type            0.000000\n",
       "name            0.000000\n",
       "elevation_ft    0.127208\n",
       "continent       0.503296\n",
       "iso_country     0.004485\n",
       "iso_region      0.000000\n",
       "municipality    0.103059\n",
       "gps_code        0.255016\n",
       "iata_code       0.833155\n",
       "local_code      0.479147\n",
       "coordinates     0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport_code_data.isnull().sum()/df_airport_code_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<p style=\"color:blue;\"> Proportion of the potential NULL values in the dataset in some cases is high like iata_code. However, this is information related to airports, every row droped just eliminate the possibility of airport matching with other tables. I would not get rid off null values in the rows </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Data Assesment\n",
    "After exploring the datasets previously, by identifying duplicates, unique and NaN values, I have the following diagnostic per dataset:\n",
    "\n",
    "#### I-94 Immigration Dataset\n",
    "\n",
    "- This contains per row a visitor (in/out) in USA Airports, here I would not remove any rows, even if there are some rows with null values\n",
    "- This will work as a fact table in our solution\n",
    "- The following labels values will be the meaning of our dimensional tables: Countryi94cntyl), Port(i94prtl), Mode(i94model), Addr(i94addrl), Type(I94VISA)\n",
    "\n",
    "#### Airport Code Dataset\n",
    "\n",
    "- This dataset contains all the airports and we will not drop any row from here to keep all the airport data, we are also sure that the iddent column is unique and it will serve as primary key\n",
    "- This will be a fact table\n",
    "\n",
    "#### USA City Demographics Dataset\n",
    "\n",
    "- This dataset will help us to underatdn the demographics of our travalers and it will be a fact table \n",
    "\n",
    "#### World Temperature Dataset\n",
    "\n",
    "- Here we can eliminate the rows of that do not offer a the temperature, in order to reduce the dataset size. However, the proportu=ion is low, the I left this elimination as optional\n",
    "- This will be another fact table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Data Model\n",
    "The data model consists on the following tables:\n",
    "\n",
    "#### Fact Tables\n",
    "- immigration\n",
    "- us_cities_demographics\n",
    "- airport_codes\n",
    "- world_temperature\n",
    "\n",
    "#### Dimensional Tables\n",
    "- i94cit_res\n",
    "- i94port\n",
    "- i94mode\n",
    "- i94addr\n",
    "- i94visa\n",
    "\n",
    "#### Considerations & Notes\n",
    "- The following tables are distributed across all nodes(DISTSTYLE ALL): i94cit_res, i94port, i94mode, i94addr, i94visa, us_cities_demographics\n",
    "- Redundancy -> DISTSTYLE ALL will copy the data of your table to all nodes - to mitigate data transfer requirement across nodes. You can find out the size of your table and Redshift nodes available size, if you can afford to copy table multiple times per node. \n",
    "\n",
    "\n",
    "#### Conceptual Data Model Diagram\n",
    "\n",
    "\n",
    "<img src=\"notebook_images/model.png\" width=\"950\" height=\"950\">\n",
    "\n",
    "\n",
    "#### Table Definitions (Details)\n",
    "\n",
    "```sql\n",
    "create_table_immigration = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS public.immigration (\n",
    "    cicid FLOAT PRIMARY KEY,\n",
    "    i94yr FLOAT SORTKEY,\n",
    "    i94mon FLOAT DISTKEY,\n",
    "    i94cit FLOAT REFERENCES i94cit_res(code),\n",
    "    i94res FLOAT REFERENCES i94cit_res(code),\n",
    "    i94port CHAR(3) REFERENCES i94port(code),\n",
    "    arrdate FLOAT,\n",
    "    i94mode FLOAT REFERENCES i94mode(code),\n",
    "    i94addr VARCHAR REFERENCES i94addr(code),\n",
    "    depdate FLOAT,\n",
    "    i94bir FLOAT,\n",
    "    i94visa FLOAT REFERENCES i94visa(code),\n",
    "    count FLOAT,\n",
    "    dtadfile VARCHAR,\n",
    "    visapost CHAR(3),\n",
    "    occup CHAR(3),\n",
    "    entdepa CHAR(1),\n",
    "    entdepd CHAR(1),\n",
    "    entdepu CHAR(1),\n",
    "    matflag CHAR(1),\n",
    "    biryear FLOAT,\n",
    "    dtaddto VARCHAR,\n",
    "    gender CHAR(1),\n",
    "    insnum VARCHAR,\n",
    "    airline VARCHAR,\n",
    "    admnum FLOAT,\n",
    "    fltno VARCHAR,\n",
    "    visatype VARCHAR\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "create_us_cities_demographics = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS public.us_cities_demographics (\n",
    "    city VARCHAR,\n",
    "    state VARCHAR,\n",
    "    median_age FLOAT,\n",
    "    male_population INT,\n",
    "    female_population INT,\n",
    "    total_population INT,\n",
    "    number_of_veterans INT,\n",
    "    foreign_born INT,\n",
    "    average_household_size FLOAT,\n",
    "    state_code CHAR(2) REFERENCES i94addr(code),\n",
    "    race VARCHAR,\n",
    "    count INT\n",
    ")\n",
    "DISTSTYLE ALL\n",
    "\"\"\"\n",
    "\n",
    "create_airport_codes = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS public.airport_codes (\n",
    "    ident VARCHAR,\n",
    "    type VARCHAR,\n",
    "    name VARCHAR,\n",
    "    elevation_ft FLOAT,\n",
    "    continent VARCHAR,\n",
    "    iso_country VARCHAR,\n",
    "    iso_region VARCHAR,\n",
    "    municipality VARCHAR,\n",
    "    gps_code VARCHAR,\n",
    "    iata_code VARCHAR,\n",
    "    local_code VARCHAR,\n",
    "    coordinates VARCHAR\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "create_world_temperature = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS public.world_temperature (\n",
    "    dt DATE,\n",
    "    AverageTemperature FLOAT,\n",
    "    AverageTemperatureUncertainty FLOAT,\n",
    "    City VARCHAR,\n",
    "    Country VARCHAR,\n",
    "    Latitude VARCHAR,\n",
    "    Longitude VARCHAR\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "create_i94cit_res = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS public.i94cit_res (\n",
    "    code FLOAT PRIMARY KEY,\n",
    "    country VARCHAR\n",
    ")\n",
    "DISTSTYLE ALL\n",
    "\"\"\"\n",
    "\n",
    "create_i94port = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS public.i94port (\n",
    "    code CHAR(3) PRIMARY KEY,\n",
    "    port VARCHAR\n",
    ")\n",
    "DISTSTYLE ALL\n",
    "\"\"\"\n",
    "\n",
    "create_i94mode = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS public.i94mode (\n",
    "    code FLOAT PRIMARY KEY,\n",
    "    mode VARCHAR\n",
    ")\n",
    "DISTSTYLE ALL\n",
    "\"\"\"\n",
    "\n",
    "create_i94addr = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS public.i94addr (\n",
    "    code CHAR(2) PRIMARY KEY,\n",
    "    addr VARCHAR\n",
    ")\n",
    "DISTSTYLE ALL\n",
    "\"\"\"\n",
    "\n",
    "create_i94visa = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS public.i94visa (\n",
    "    code FLOAT PRIMARY KEY,\n",
    "    type VARCHAR\n",
    ")\n",
    "DISTSTYLE ALL\n",
    "\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "#### Mapping Out Data Pipelines and Data Quality checks\n",
    "The DAG shown in the graph shows the nodes and how the data and tables are being loaded with the data. In our particular case every table (facts or dimentions)\n",
    "has a data check of the type count.\n",
    "\n",
    "<img src=\"notebook_images/dag.png\" width=\"950\" height=\"950\">\n",
    "\n",
    "\n",
    "#### Here some wuality checks examples in dimensional tables.\n",
    "\n",
    "\n",
    "```{'name': 'i94cit_res',\n",
    "     'value': 'i94cntyl',\n",
    "     'columns': ['code', 'country'],\n",
    "     'dq_checks': [{'check_sql': \"SELECT COUNT(*) FROM i94cit_res WHERE code is null\", 'expected_result': 0}]\n",
    "     },\n",
    "    {'name': 'i94visa',\n",
    "     'value': 'I94VISA',\n",
    "     'columns': ['code', 'type'],\n",
    "     'dq_checks': [{'check_sql': \"SELECT COUNT(*) FROM i94visa WHERE code is null\", 'expected_result': 0}]\n",
    "     },\n",
    "    {'name': 'i94port',\n",
    "     'value': 'i94prtl',\n",
    "     'columns': ['code', 'port'],\n",
    "     'dq_checks': [{'check_sql': \"SELECT COUNT(*) FROM i94port WHERE code is null\", 'expected_result': 0}]\n",
    "     },\n",
    "    {'name': 'i94addr',\n",
    "     'value': 'i94addrl',\n",
    "     'columns': ['code', 'addr'],\n",
    "     'dq_checks': [{'check_sql': \"SELECT COUNT(*) FROM i94addr WHERE code is null\", 'expected_result': 0}]\n",
    "     },\n",
    "    {'name': 'i94mode',\n",
    "     'value': 'i94model',\n",
    "     'columns': ['code', 'mode'],\n",
    "     'dq_checks': [{'check_sql': \"SELECT COUNT(*) FROM i94mode WHERE code is null\", 'expected_result': 0}]\n",
    "}```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## How to run the ETL to model the data\n",
    "\n",
    "1. Clone the repository and fill the credential information in tables/dwh.cfg and dags/dw.cfg\n",
    "2. Read the file dags/datasets/README.md (It will tell you about the datasets needed)\n",
    "3. Upload those datasets in a S3 bucket\n",
    "4. Please follow the instructions to run Apache Airflow in a Docker container [Instructions](https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html))\n",
    "5. **Apache AirFlow 2.0.1 has an error in the official documentation, and I created a video to fix it and \n",
    "share it with the world (Sharing is caring) [FIX THAT BUG](https://youtu.be/RVKRtgDIh8A))**\n",
    "6. Go to the main folder project and is Apache AirFlow is not running do:\n",
    "\n",
    "```\n",
    "docker compose up  (Start the services)\n",
    "docker compose down (Stop the services)\n",
    "```\n",
    "7. Configure the connector in Apache Airflow to be able to see Amazon Redshift [Detailed Steps](https://www.progress.com/tutorials/jdbc/connect-to-redshift-salesforce-and-others-from-apache-airflow)\n",
    "8. You should be able to see the DAG -> immigration_etl_dag in ApacheAirflow\n",
    "9. **Run the script tables/create_tables.py**\n",
    "10. Finally, in Apache AirFlow you can execute the dag and wait for the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Project Write Up, Questions & Assumptions\n",
    "\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "\n",
    " <p style=\"color:blue;\">In terms of technologies, I wanted to bring also some technologies not touched by the certification like docker containers.\n",
    "Amazon RDS,  Redshift are the idea tools for loading data into databases. S3 buckets are quite convenient to storage large datasets in the cloud.\n",
    "Finally, an orchestrator like Apaceh AirFlow plays a main role to not only execute all the ETLs but also being able to monitor preformance and have logs on executions.</p>\n",
    "\n",
    "* Propose how often the data should be updated and why.\n",
    " <p style=\"color:blue;\">If the dta sources are being updated very often a short time window should be specified, like every day at 5 am. However, the way that I have design this ETL based\n",
    "on the data freshness and datasets that are not updated in realtime, it would be ideal to have monthly reports or updates. In some cases it may be conveninent weekly reports.\n",
    "\n",
    "I would not go beyond for a month since that can bring data innacuracy if data consumers are building prediction models or any kind of analysis on inmmigration behaviour.</p>\n",
    "\n",
    "\n",
    "### Write a description of how you would approach the problem differently under the following scenarios:\n",
    " \n",
    " * The data was increased by 100x.\n",
    " \n",
    "<p style=\"color:blue;\">If the data is storaged in the cloud then the use of Spark in a EMR (Virtual machine) in Amazon can help to cope with the load. This is a modular and scalable approach. We can also split the dag by using partitioning functionality in AirFlow (divide and conquer) </p>\n",
    "\n",
    " \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " <p style=\"color:blue;\">This is a very common case in the industry, fortunatelly in Airflow we can use the internal scheduler to make a cron syntax to specify in the DAG to run at 7am every day. </p>\n",
    " \n",
    " \n",
    " * The database needed to be accessed by 100+ people.\n",
    " \n",
    " <p style=\"color:blue;\">Fortunatelly Amazon Redshift as Data Warehouse solution is designed to serve different data consumers (data analysts, marketers, etc) and not everybody should have access to everything. That is great feature to make different roles in Redshift and provide particular access to certain fact tables or perform as well certain operations. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'Capstone_Project.ipynb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
